%{\let\clearpage\relax \chapter{Related Work}}
\chapter{Related Work}
\label{cha:Related Work}

Since this thesis will employ reinforcement learning in the training of the autonomous driving agent it is important to review the current state of the art in reinforcement learning and autonomous driving. Some techniques of the researched works might not be applicable in this thesis due to the smaller scope and availible resources of this thesis, it is still important to review them to understand the current state of the art and to be able to utilise aspects of them. Simulation environments for reinforcement learning and self driving will also be reviewed, since they play a bit role in the state of the art of both fields and will be employed in this thesis.
Finally some papers about the use of reinforcement learning for self driving will be reviewed.


\section{Reinforcement Learning}

Reinforcement learning algorithms have been around for a long time, but only recently have they been able to achieve superhuman performance in games and control tasks. Most reinforcement learning algorithm formalize the problem using a state space and an action space and a reward function associated with state transitions. The RL algorithms are built to select an action in a given state and try to maximize the cummulative reward along the state transitions. This process of selecting an action is called the policy.
The classical version of the Q-learning RL algorithm keeps a table of state-action pairs and their associated quality values which are learned/computed during training. The amount of state-action pairs can be very large and not feasible to compute in many cases, especially for environments with continuous state and action spaces. An example of a continuous state space would be the position of an object in a continuous 2D space. %TODO explain how q learning learns the state-action value table?
%also explain how deep q learning learns (similar to the q learning table process)
To solve this problem neural networks have been used to approximate the Q-values, they take a representation of a state from the state space as input. This approach is called Deep Q-Learning and has been used to achieve superhuman performance in many games \autocite{atari}. The initial success of Deep Q-Learning has lead to many improvements and variations of the algorithm, since then RL algorithms have proved to be very useful in a wide range of applications. %TODO cite protein folding and others?


A mayor weakness of the Deep Q-learning algorithm was its stability, the parameter updates of the Q-learning agent's neural network can result in a collapse in performance during training. The Proximal Policy Optimization and other algorithms have been developed to improve the stability of the training process. The PPO \autocite{ppo} algorithm restricts the parameter updates to a small region in the TODO space, this ensures the policy cannot change drastically and improves stability.





AlphaGo (most famous example of combining reinforcement learning with MCTS) (mention the many improvements that came afterwards)


\section{Self Driving}

Papers that use Carla

Tesla \autocite{teslaEndToEnd}


Mention experiments from PopCulture/YouTube

Mention immitation learning (supervised) for autonomous driving

\section{Simulation for Reinforcement Learning (and Self Driving)}

explain that simulations often make things available that are not available in the real world (e.g. perfect sensor data, perfect physics, perfect control over the environment and ground truths (object detection bounding boxes))

Gym environments, unity as a simulation environment, carla, collimator.ai (development platform)

https://carla.org/

\autocite{carla}

https://microsoft.github.io/AirSim/
\autocite{airsim}


\section{Reinforcement Learning for Self Driving}


TÃ¼bingen:
% https://www.youtube.com/watch?v=GYnlqiSqZiU&list=PL05umP7R6ij321zzKXK6XCQXAaaYjQbzr&index=13



TODO reference papers that use the improvements I propose

Key Papers in RL:
https://spinningup.openai.com/en/latest/spinningup/keypapers.html#bonus-classic-papers-in-rl-theory-or-review
