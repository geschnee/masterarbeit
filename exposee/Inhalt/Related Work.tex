%{\let\clearpage\relax \chapter{Related Work}}
\chapter{Related Work}
\label{cha:Related Work}

Since this thesis will employ reinforcement learning in the training of the autonomous driving agent it is important to review the current state of the art in reinforcement learning and autonomous driving. Some techniques of the researched works might not be applicable in this thesis due to the smaller scope and availible resources of this thesis, it is still important to review them to understand the current state of the art and to be able to utilise aspects of them. Simulation environments for reinforcement learning and self driving will also be reviewed, since they play a bit role in the state of the art of both fields and will be employed in this thesis.
Finally some papers about the use of reinforcement learning for self driving will be reviewed.


\section{Reinforcement Learning}

Reinforcement learning algorithms have been around for a long time, but only recently have they been able to achieve superhuman performance in games and control tasks. Most reinforcement learning algorithm formalize the problem using a state space and an action space and a reward function associated with state transitions. The RL algorithms are built to select an action in a given state and try to maximize the cummulative reward along the state transitions. This process of selecting an action is called the policy.

The classical version of the Q-learning RL algorithm keeps a table of state-action pairs and their associated quality values which are learned/computed during training. These Q-values are used by the algorithms' policy. The amount of state-action pairs can be very large and not feasible to compute in many cases, especially for environments with continuous state and action spaces. %An example of a continuous state space would be the position of an object in a continuous 2D space. 

To solve this problem neural networks have been used to approximate the Q-values, they take a representation of a state from the state space as input. This approach is called Deep Q-Learning, convolutional neural networks are often used \autocite{atari}. The initial success of Deep Q-Learning has lead to many improvements and variations of the algorithm, since then RL algorithms have proved to be very useful in a wide range of applications.


A mayor weakness of the initial Deep Q-learning algorithm was its stability during training, the parameter updates of the Q-learning agent's neural network can result in a collapse in performance. Proximal Policy Optimization (PPO) and other algorithms have been developed to improve the training process stability. The PPO \autocite{ppo} algorithm restricts the size of policy changes caused by parameter updates, this ensures the policy cannot change drastically and improves stability. PPO is currently one of the most popular algorithms for reinforcement learning and has already been successfully used in the domain of autonomous driving \autocite{maximilian}.
% other algorithms with better training stability are Double DQN, TRPO
%TODO maybe introduce TRPO as it was the predecessor of PPO


Another major improvement in the domain of reinforcement learning was the combination of neural networks with traditional planning and search algorithms, the most famous example for this is AlphaGo \autocite{alphago}. The search algorithms (typically Monte Carlo Tree Search) evaluate the possible actions at a given state. The neural networks are used for the evaluation of states and actions in the search algorithms. AlphaGo was developed for a 2 player deterministic game with a discrete state and action space. Since then the combination of neural networks and search algorithms has also been used for all kinds of problems, for example single player continuous state and action spaces \autocite{alphagoimprovementmuzero}.




\section{Self Driving}

As mentioned before there has been a lot of progress in the domain of self driving in recent years, often driven by private enterprises that might not publish their findings in academic journals. Sophisticated self driving algorithms often consist of many components to achive satisfying performance, Tesla FSD for example uses separate object detection, occupancy and planning components \autocite{teslaEndToEnd}.
This thesis builds directly upon the work of \autocite{jonas_koenig}, \autocite{merlin_flach} and \autocite{maximilian}. \autocite{jonas_koenig} built a self driving agent that was trained to avoid collisions in a simulated arena using an evolutionary approach to neural network training, the agent used visual inputs that were preprocessed before feeding it to the neural network. \autocite{merlin_flach} investigated the feasibility of transfering the agent to the real world, this research showcased many difficulties, most notably the object recognition part of the agent. \autocite{maximilian} investigated a different task than the two previous papers, the agent was trained to pass a parcour by driving through a sequence of goals. This task is identical to the one investigated here. \autocite{maximilian} successfully used PPO to train the agent which was fed preprocessed data similar to \autocite{jonas_koenig}.

Another interesting approach to building self driving agents is using imiatation learning \autocite{imitation_learning}, imitation learning agents learn to mimic the behaviour exhibited in the training set. The training set is usually generated by humans driving the car, this approach is rumored to be used by Tesla to train their self driving agents \autocite{teslaEndToEnd}.


% \autocite{autonomous_vehicles_review} %TODO read this paper and cite it

%https://safe-intelligence.fraunhofer.de/en/autonomous-driving

%Papers that use Carla



%Mention experiments from PopCulture/YouTube


\section{Simulation for Reinforcement Learning (and Self Driving)}

Simulations play a huge role in reinforcement learning and the development of self driving agents. Simulations provide a number of benefits over real world experiments. They are much cheaper to run, they can be run in parallel and they can be run much faster than real world experiments. In additon the programmers have direct and perfect control over the environment. This allows for much faster experimentation and training of reinforcement learning agents. Simulations also allow for the creation of scenarios that are not possible in the real world, this is especially useful for reinforcement learning agents that are trained to avoid collisions. Simulations also allow for the creation of ground truths, which are often not available in the real world such as perfect sensor data and object bounding boxes. Ground truths are used to evaluate the performance of the agent and can be used to train the agent. In addition the physics of the simulation can be modified, for example it is possible to increase the simulation speed.

Simulated environments, especially games have served as baselines for reinforcement learning algorithms. The most famous baseline are the atari games \autocite{atari}. The need for easy use of reinforcement learning algorithms has lead to the development and adoption of the Gynasium API \autocite{gymnasium}, the Gymnasium API defines an interface that can be used to model tasks as reinforcement learning problems. This interface is called a Gym environment, an allows for easy testing of reinforcement learning algorithms on a wide range of tasks. Furthermore the Gymnasium API allows for easy comparison of different algorithms on the same task. A wide range of reinforcement learning frameworks support the Gymnasium API, for example Google's dopamine \autocite{dopamine} and OpenAI's baselines \autocite{sb3}.

Wrappers for Gymnasium environments are often used to facilitate the use of more advanced simulations for environments, such as for example the game engines Unity and Unreal or the physics simulator MuJoCo \autocite{mujoco}. The complexity and interest in self driving has also lead to the development of dedicated simulators, such as the Carla \autocite{carla} and the AirSim \autocite{airsim} simulator. The Carla simulator provides researchers with useful features such as weather control and ground truths for object detection and segmentation.

There are also dedicated frameworks for reinforcement learning that directly integrate with game engines, such as the ML-Agents framework \autocite{mlagents} for Unity.\autocite{maximilian} used this framework to train the self driving agent.

% TODO check Ray: https://docs.ray.io/en/latest/rllib/index.html
% TODO check dopamine https://github.com/google/dopamine


\section{Reinforcement Learning for Self Driving}

\autocite{drl_for_ad} review the use of reinforcement learning for autonomous driving, they also describe many improvements for reinforcement learning algorithms that can improve the training stability and performance, such as for example reward shaping.
In addition to published research papers there have been a lot of experiments, tutorials and demonstrations of self driving agents on YouTube and GitHub. The University of Tübingen published their full lecture series on Self-Driving Cars \autocite{tuebingen}, the series also includes a section on reinforcement learning.

% papers that cite carla: https://scholar.google.de/scholar?cites=660591080772510291&as_sdt=2005&sciodt=0,5&hl=de


% Tübingen: % https://www.youtube.com/watch?v=GYnlqiSqZiU&list=PL05umP7R6ij321zzKXK6XCQXAaaYjQbzr&index=13



% TODO reference papers that use the improvements I propose

% Key Papers in RL: https://spinningup.openai.com


