%\chapter{Methods}
%\label{cha:Methods}
%\lipsum \autocite{DBLP:books/sp/HarderR01}

\section{Investigating the feasibility of transfering the policy to a physical robot.}

Previous sections describe how a policy was developed that can be used to control an agent in a simulated environment. This section describes how I will investigate the feasibility of transfering the developed policy onto physical devices. The simulated agent was modeled after a Nvidia JetBot. The Nvidia JetBot is a small robot that is equipped with a camera, a processing unit and two motors that can be controlled independently. 
The Nvidia Jetbot is designed to be able to execute AI software. However the limited computational power of the JetBot raises the question whether the developed policy can be transfered to the JetBot. 

The developed policy takes a camera image from the front of the agent as input and applies preprocessing steps to the image. The preprocessed image is then processed by a convolutional neural network that outputs two acceleration values, one for each motor. The acceleration values are applied to the motors for a fixed amount of time. While the agent is moving, the camera image is constantly updated and the policy is applied to the new image. It is crucial that the agent's policy can be computed quick enough to be able act in real time. 

\subsection{effects of insufficiently slow policy computation}
If the jetbot is not able to compute the developed policy in the required time, there are two options that do not require changes to the developed/trained policy. The first option is to stop the motors until the policy is computed and then apply the acceleration values. This would make the jetbot movement overall slower and less smooth.
The second option is to apply the last computed acceleration values to the motors until the new policy is computed. This could result in a degradation of the jetbot's performance since the actions would be less accurate.
The two options are shown with visual representations in \ref{fig:slow_policy_computation}.


% gif splitter
% https://ezgif.com/split



\newcommand{\spc}[2]{\subfigure[#1]{\includegraphics[width=0.2\textwidth]{Bilder/slow_policy_computation/#2.png}}}
%\newcommand{\spc}[2]{\begin{subfigure}{.5\textwidth}\centering\includegraphics[width=0.2\textwidth]{Bilder/slow_policy_computation/#2.png}\caption*{#1}\end{subfigure}}
% trying to remove a), b) ... https://tex.stackexchange.com/questions/165508/remove-a-b-from-subfigure-numbering-but-keep-the-subfigure-caption

\begin{figure}
    
    \begin{center}
    \begin{tabular}{|| c | c | c ||} 
        \hline
        Policy computation in time & \makecell{Option 1: \\ Wait} & \makecell{Option 2:\\ Apply previous outputs}  \\ [0.5ex] 
        \hline\hline
        \spc{Start}{start} &  \spc{Start}{start} & \spc{Start}{start} \\ 
        \hline
        \spc{Agent turns right}{agent_turns_right} & \spc{Agent turns right}{agent_turns_right} & \spc{Agent turns right}{agent_turns_right} \\
        \hline
        \spc{Agent stops turning and goes strait}{agent_turns_left} & \spc{Agent waits}{agent_turns_right} & \spc{Agent continues to turn}{agent_fails_to_turn_left} \\
        \hline
        \spc{Agent continues}{agent_continues_properly}  & \spc{Agent stops turning and goes strait}{agent_turns_left} & \spc{Agent crashes}{agent_crashes} \\
        \hline
        \makecell{Agent moves properly.}  & \makecell{Agent overall speed is reduced.} & \makecell{Agent behaviour is changed.} \\
        \hline
    \end{tabular}
    \end{center}
    \caption{Possible effects of slow policy computation on the performance.}
    \label{fig:slow_policy_computation}
\end{figure}



\subsection{Experiments - Policy Replay}

This experiment examines the processing capabilities of the Nvidia Jetbot in the context of policy computation. The goal of this experiment is to determine if the policy can be computed on the jetbot hardware in real time. This would allow for a transfer of the developed policy to the physical agent without resorting to the two options highlighted in the previous section.

The experiment is conducted by recording the agent's actions in simulation and replaying them on the Nvidia Jetbot. The recordings consist of the
camera images and the acceleration values that the policy outputs while the agent is moving in the simulated environment. The time it takes to replay the recorded steps on the jetbot is measured. The measured times are compared against the $fixedTimestepsLength$ parameter of the recorded data.
The replay of a step consists of all the processing to obtain new acceleration values from a camera image. This is the image preprocessing and memory mechanism as specified by the $image\_preprocessing$ and $frame\_stacking$ environment parameters. Together with the neural network inference. 
Multiple games of all difficulty and light settings are recorded to have a diverse set of data to replay. The recording and replay of an episode are shown in pseudocode \ref{pseudocode:record_episode} and \ref{pseudocode:replay_episode}.

\renewcommand{\thepseudonum}{\roman{pseudonum}}
\begin{pseudocode}{Record Episode}{ }
\COMMENT{Record episode}\\

\PROCEDURE{record\_episode}{policy, env, directory}

env.\CALL{RESET}{}\\

sampled\_actions \GETS [] \\
infer\_obsstrings \GETS [] \\
step\_obsstrings \GETS [] \\

done \GETS \FALSE\\

\WHILE ! done \DO
\BEGIN
obs, obsstring \GETS env.\CALL{GetObservation}{}\\
action \GETS policy.\CALL{Infer}{obs}\\

step\_obsstring, done \GETS env.\CALL{step}{action}\\

infer\_obsstrings.\CALL{append}{obsstring}\\
step\_obsstrings.\CALL{append}{step\_obsstring}\\
sampled\_actions.\CALL{append}{action}\\
\END\\

\FOR i \GETS 0 \TO len(step\_obsstrings) \DO
    \CALL{SaveToFile}{step\_obsstrings[i], directory + "/step\_image" + i + ".png"}\\
\FOR i \GETS 0 \TO len(infer\_obsstrings) \DO
    \CALL{SaveToFile}{infer\_obsstrings[i], directory + "/infer\_image" + i + ".png"}\\
\FOR i \GETS 0 \TO len(sampled\_actions) \DO
    \CALL{SaveToFile}{sampled\_actions[i], directory + "/sampled\_action" + i + ".npy"}\\


\ENDPROCEDURE
\label{pseudocode:record_episode}
\end{pseudocode}

\renewcommand{\thepseudonum}{\roman{pseudonum}}
\begin{pseudocode}{Replay Episode}{ }
\COMMENT{Replay episode and record processing + inference time}\\

\PROCEDURE{replay\_episode}{ policy, env, directory}

recorded\_episode\_length \GETS \CALL{LoadRecordedEpisodeLength}{directory}\\
recorded\_actions \GETS \CALL{LoadRecordedActions}{directory}\\
infer\_obs\_unity\_images \GETS \CALL{LoadInferImages}{directory}\\
step\_obs\_unity\_images \GETS \CALL{LoadStepImages}{directory}\\

reproduce\_times \GETS [] \\

replay\_time\_start \GETS \CALL{TIME}{}\\

\FOR i \GETS 0 \TO recorded\_episode\_length \DO
\BEGIN
obs \GETS \CALL{ProcessImage}{env, infer\_obs\_unity\_images[i]}\\
action \GETS policy.\CALL{Infer}{obs}\\
reproduce\_times.\CALL{append}{\CALL{TIME}{} - replay\_time\_start}\\

\CALL{AssertEqual}{action, recorded\_actions[i]}\\

replay\_time\_start \GETS \CALL{TIME}{}\\

\CALL{ProcessImage}{env, step\_obs\_unity\_images[i]}\\
\END\\
\RETURN{reproduce\_times}
\ENDPROCEDURE

\label{pseudocode:replay_episode}
\end{pseudocode}


% we always record in the use_fresh_obs mode




More text down here, why is it not loading?




