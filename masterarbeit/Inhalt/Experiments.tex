\chapter{Experiments}
\label{cha:Experiments}

\section{Evaluation metrics}

\subsection{success\_rate}
The primary metric of evaluation for the developed agents is the $success\_rate$. The $success\_rate$ is defined as the ratio of successful episodes to the total number of episodes. An episode is considered successful if the agent passes through all three goals within the time limit \ref{time_limit}. Collisions of the agent do not disqualify an episode from being succesful, as long as the agent passes all goals.

\subsection{goal\_completion\_rate}

The $goal\_completion\_rate$ is defined as the ratio of passed goals to the total number of goals in the episodes. The $goal\_completion\_rate$ is a more fine-grained metric than the $success\_rate$. However the two metrics are closely related as a high $success\_rate$ implies a high goal\_completion\_rate. The major advantage of the $goal\_completion\_rate$ is that it can used to measure the progress of an agent during training more accurately. 
The $goal\_completion\_rate$ would increase when the agent is able to pass more goals on average, whereas the $success\_rate$ would only increase when the agent is able to pass all goals in an episode more often. The $goal\_completion\_rate$ captures learning progress earlier in training. This behaviour can be observed in training runs, shown in \ref{fig:success_rate_vs_goal_completion_rate}.


\begin{figure}
    \centering
    \subfigure[success\_rate]{\includegraphics[width=0.3\textwidth]{Bilder/metrics/sr_vs_gcr_success_rate.PNG}}
    \subfigure[goal\_completion\_rate]{\includegraphics[width=0.3\textwidth]{Bilder/metrics/sr_vs_gcr_goal_completion_rate.PNG}}
    \caption{Difference in success rate and goal completion rate during early stages of training.}
    \label{fig:success_rate_vs_goal_completion_rate}
\end{figure}
% diese Bilder funktionieren nur für frühe Episoden, später sind die Werte sehr gleich


\section{Standard evaluation runs}

The agent is evaluated on every combination of light and difficulty settings. The $success\_rate$ metric is collected for each combination of light and difficulty settings separately. The collected $success\_rates$ are then averaged to produce aggregate $success\_rates$ for each light setting and difficulty setting. The 3 light and difficulty settings result in a total of 9 evaluated combinations and $success\_rates$. A total of 7 aggregate $success\_rates$ are computed, 3 for each light setting and 3 for each difficulty setting. The final aggregate is the $total\_success\_rate$, the average of all 9 collected $success\_rates$. 
The $total\_success\_rate$ is used to determine the overall best agent and model.


\begin{table}
\begin{center}
\resizebox{\textwidth}{!}{%
\begin{tabular}{|| c | c | c | c | c ||} 
    \hline
    \makecell{} & bright & standard  & hard & \makecell{aggregate \\ success\_rates} \\ [0.5ex] 
    \hline\hline
    \makecell{easy} &  success\_easy\_bright & success\_easy\_standard & success\_easy\_dark & success\_easy \\ 
    \hline
    \makecell{medium} &  success\_medium\_bright & success\_medium\_standard & success\_medium\_dark & success\_medium \\ 
    \hline
    \makecell{hard} &  success\_hard\_bright & success\_hard\_standard & success\_hard\_dark & success\_hard \\ 
    \hline
    \makecell{aggregate \\ success\_rates}  & success\_bright & success\_standard & success\_dark & total\_success\_rate \\
    \hline
\end{tabular}}
\end{center}
\caption{Collected and aggregate success\_rate metrics}
\label{table:success_rates_system}
\end{table}


The agent is evaluated by running a fixed number of episodes for the specific light and difficulty settings. The amount of episodes that the agent is evaluated on is defined by the config parameter $n\_eval\_episodes$. As described in \ref{cha:env_description}, each difficulty setting includes a number of unique tracks. Furthermore the initial starting position/rotation is parameterized by the config parameter $spawn\_point$.
The tracks and starting positions for the agent are generated by the algorithm shown in \ref{pseudocode:generate_track_rotation}. The algorithm divides the random interval specified by the spawn point parameter into $n\_eval\_episodes$ equal parts. These spawn rotations are then each assigned a track in repeating order.
This algorithm ensures that the agent is evaluated on the full range of unique tracks and spawn points.
It also makes the evaluations comparable, as the same combinations of tracks and spawn points are used for each agent evaluation.



% TODO generate_track_rotation




\subsection{Deterministic check}

deter or non-deter better

\subsection{identical start condition}

see if the policy is consistent when it is run multiple times with the same start condition



\subsection{Fresh obs vs non-fresh obs}

% ## fresh obs better than nonfresh obs
 
% ### models trained on nonfresh obs

% fresh obs are better than non-fresh obs

% ### models trained on fresh obs

% TODO feasible (time?)


\subsection{policy timestepLength generalization}

policy timestepLength generalization 

what happens to a model trained on fixedTimestepsLength when it is evaluated with a shorter/longer/unrestricted fixedTimestepsLength